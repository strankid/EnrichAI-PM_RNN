{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3870, 418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/apoorv/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:3694: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72 samples, validate on 2970 samples\n",
      "Epoch 1/10\n",
      "72/72 [==============================] - 2s 30ms/step - loss: 1.3002 - acc: 0.5000 - val_loss: 0.6753 - val_acc: 0.6572\n",
      "Epoch 2/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.7192 - acc: 0.6944 - val_loss: 0.6098 - val_acc: 0.8461\n",
      "Epoch 3/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.4490 - acc: 0.8194 - val_loss: 0.5841 - val_acc: 0.8047\n",
      "Epoch 4/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4360 - acc: 0.8056 - val_loss: 0.5687 - val_acc: 0.7882\n",
      "Epoch 5/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3833 - acc: 0.8056 - val_loss: 0.5817 - val_acc: 0.7135\n",
      "Epoch 6/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2376 - acc: 0.9167 - val_loss: 0.5838 - val_acc: 0.6653\n",
      "Epoch 7/10\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3650 - acc: 0.8194 - val_loss: 0.5667 - val_acc: 0.6912\n",
      "Epoch 8/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2713 - acc: 0.8750 - val_loss: 0.5755 - val_acc: 0.6791\n",
      "Epoch 9/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.3148 - acc: 0.8333 - val_loss: 0.6102 - val_acc: 0.6269\n",
      "Epoch 10/10\n",
      "72/72 [==============================] - 1s 8ms/step - loss: 0.2571 - acc: 0.8889 - val_loss: 0.6469 - val_acc: 0.5734\n",
      "Test loss: 0.6468585929485282\n",
      "Test accuracy: 0.5734006736013625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected cu_dnnlstm_input to have 3 dimensions, but got array with shape (0, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1ed66c9dfead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mheads_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0;31m# Validate and standardize user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1486\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected cu_dnnlstm_input to have 3 dimensions, but got array with shape (0, 1)"
     ]
    }
   ],
   "source": [
    "#things to be fixed in the model: clear sequence if gap greater than 2 days\n",
    "#look forward for one day\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "VALIDATION_HEADS = 12\n",
    "SEQ_LEN = 10\n",
    "EPOCHS = 10  \n",
    "BATCH_SIZE = 10 \n",
    "NAME = f\"{SEQ_LEN}-SEQ-{BATCH_SIZE}-BATCH-{int(time.time())}\"\n",
    "\n",
    "\n",
    "\n",
    "def prepare(df):\n",
    "\tdf.drop(['Event','Before PM','After PM','end_date','Nozzle-A','Nozzle-B','Nozzle-C','Nozzle-D','Nozzle-E','Nozzle-F','Nozzle-G','Nozzle-H','Nozzle-I','Nozzle-J','Nozzle-K','Nozzle-L','Nozzle-M','Nozzle-N','Nozzle-O','Nozzle-P','Nozzle-Q','Nozzle-R','Nozzle-S','Nozzle-T','Nozzle-U','Nozzle-V','Nozzle-W','Nozzle-X','Nozzle-Y','Nozzle-Z'],axis=1,inplace=True)\n",
    "\tdf.start_dt = pd.to_datetime(df.start_dt)\n",
    "\tdf.sort_values(['start_dt'],inplace=True)\n",
    "\tdf.iloc[:,4:-1] = preprocessing.RobustScaler().fit_transform(df.iloc[:,4:-1])\n",
    "\t\n",
    "\n",
    "\treturn df\n",
    "\n",
    "def preprocess(df,heads, validation=False):\n",
    "\n",
    "\tsequential_data = []\n",
    "\tprev_days = deque(maxlen=SEQ_LEN)\n",
    "\n",
    "\tfor head in heads:\n",
    "\t\tdata = df[df['head_id']==head]\n",
    "\t\tdata.index = data.start_dt\n",
    "\t\tdata.sort_index(axis=1, inplace=True)\n",
    "\t\tdata.drop(['start_dt','end_dt','head_id','module_position'],axis=1,inplace=True) \n",
    "\t\tfor i in data.values:\n",
    "\t\t\tprev_days.append(i[:-1])\n",
    "\t\t\tif len(prev_days) == SEQ_LEN:\n",
    "\t\t\t\tsequential_data.append([np.array(prev_days), i[-1]])\n",
    "\t\tprev_days.clear()\n",
    "\n",
    "\trandom.shuffle(sequential_data)\n",
    "    print(sequential_data.shape)\n",
    "\n",
    "\tif validation==False:\n",
    "\t\tpositives = []\n",
    "\t\tnegatives = []\n",
    "\n",
    "\t\tfor seq, target in sequential_data:  \n",
    "\t\t\tif target == 0:  \n",
    "\t\t\t\tnegatives.append([seq, target])  \n",
    "\t\t\telif target == 1:  \n",
    "\t\t\t\tpositives.append([seq, target])  \n",
    "\n",
    "\t\trandom.shuffle(positives)  \n",
    "\t\trandom.shuffle(negatives)  \n",
    "\n",
    "\t\tlower = min(len(positives), len(negatives)) \n",
    "\n",
    "\t\tpositives = positives[:lower]  \n",
    "\t\tnegatives = negatives[:lower]  \n",
    "\t\n",
    "\t\tsequential_data = positives+negatives\n",
    "\t\n",
    "\t\trandom.shuffle(sequential_data)\n",
    "        print(sequential_data.shape)\n",
    "\t\n",
    "\tX = []\n",
    "\ty = []\n",
    "\n",
    "\tfor seq, target in sequential_data:  \n",
    "\t\tX.append(seq)  \n",
    "\t\ty.append(target) \n",
    "\n",
    "\treturn np.array(X), y\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"final_model_sep_27.csv\")\n",
    "df = prepare(df)\n",
    "\n",
    "heads_train = df.head_id.unique()[VALIDATION_HEADS:]\n",
    "heads_validation = df.head_id.unique()\n",
    "\n",
    "train_x, train_y = preprocess(df,heads_train)\n",
    "validation_x, validation_y = preprocess(df,heads_validation, validation=True)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(CuDNNLSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(NAME))\n",
    "\n",
    "filepath = \"RNN_Final-{epoch:02d}-{val_acc:.3f}\"  # unique file name that will include the epoch and the validation acc for that epoch\n",
    "checkpoint = ModelCheckpoint(\"models/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) # saves only the best ones\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    train_x, train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, validation_y),\n",
    "    callbacks=[tensorboard, checkpoint],\n",
    ")\n",
    "\n",
    "# Score model\n",
    "score = model.evaluate(validation_x, validation_y, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "# Save model\n",
    "model.save(\"models/{}\".format(NAME))\n",
    "\n",
    "heads_test = df.head_id.unique()[2]\n",
    "test_x, test_y = preprocess(df, heads_test)\n",
    "model.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
